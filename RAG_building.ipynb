{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from openai import OpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Needs PyPDF, langchain-community to load a PDF\n",
    "\n",
    "#function for loading the documents to pass into the RAG model\n",
    "\n",
    "def load_documents(data_path):\n",
    "    loader = PyPDFLoader(data_path)\n",
    "    document = loader.load_and_split()    #loader.load() returns a list of strings, each string is a page of the PDF. Use loader.load_and_split() instead\n",
    "    return document\n",
    "\n",
    "doc = load_documents(\"LLM MODEL SURVEY.pdf\") #Docs can be retrieved by page numbers\n",
    "\n",
    "# print(doc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 272 chunks\n",
      "page_content='overview of techniques developed to build, and augment LLMs.\n",
      "We then survey popular datasets prepared for LLM training,\n",
      "fine-tuning, and evaluation, review widely used LLM evaluation\n",
      "metrics, and compare the performance of several popular LLMs\n",
      "on a set of representative benchmarks. Finally, we conclude\n",
      "the paper by discussing open challenges and future research\n",
      "directions.\n",
      "I. I NTRODUCTION\n",
      "Language modeling is a long-standing research topic, dat-\n",
      "ing back to the 1950s with Shannonâ€™s application of informa-\n",
      "tion theory to human language, where he measured how well\n",
      "simple n-gram language models predict or compress natural\n",
      "language text [3]. Since then, statistical language modeling\n",
      "became fundamental to many natural language understanding\n",
      "and generation tasks, ranging from speech recognition, ma-\n",
      "chine translation, to information retrieval [4], [5], [6].\n",
      "The recent advances on transformer-based large language\n",
      "models (LLMs), pretrained on Web-scale text corpora, signif-' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-21T02:03:22+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-21T02:03:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LLM MODEL SURVEY.pdf', 'total_pages': 43, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "#Now, using text-splitting to get the text split from the PDF\n",
    "\n",
    "\n",
    "def split_text(pdf_path):\n",
    "    # 1. Load the PDF properly\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # 2. Create the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        #If you want bigger chunk size\n",
    "        chunk_size=1000,\n",
    "        #To overlap the context between chunks\n",
    "        chunk_overlap=200,\n",
    "        #Keep chunks above this size\n",
    "        length_function=len,\n",
    "        # You can add custom separators\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "    \n",
    "    # 3. Split the documents\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Document split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "pdf_path = \"LLM MODEL SURVEY.pdf\"\n",
    "chunks = split_text(pdf_path)\n",
    "\n",
    "print(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we have to add embedding for each of the chunks from the document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "''' when creating the embedding function, make sure to use the same embedding when creating the database \n",
    "    and also when we want to query the database'''\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Fetching the embedding function\n",
    "embeddings = get_embedding_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top relevant documents:\n",
      "1. outperforming other models like LLaMA and Stanford Alpaca\n",
      "in more than 90% of cases. 13 shows the relative response\n",
      "quality of Vicuna and a few other well-known models by\n",
      "GPT-4. Another advantage of Vicuna-13B is its relative limited\n",
      "computational demand for model training. The training cost of\n",
      "Vicuna-13B is merely $300.\n",
      "Fig. 13: Relative Response Quality of Vicuna and a few other\n",
      "well-known models by GPT-4. Courtesy of Vicuna Team.\n",
      "Like Alpaca and Vicuna, the Guanaco models [63] are also\n",
      "finetuned LLaMA models using instruction-following data. But\n",
      "the finetuning is done very efficiently using QLoRA such\n",
      "that finetuning a 65B parameter model can be done on a\n",
      "single 48GB GPU. QLoRA back-propagates gradients through\n",
      "a frozen, 4-bit quantized pre-trained language model into Low\n",
      "Rank Adapters (LoRA). The best Guanaco model outperforms\n",
      "all previously released models on the Vicuna benchmark,\n",
      "reaching 99.3% of the performance level of ChatGPT while\n",
      "2. LLAMA 2 70B - 87.33\n",
      "LLAMA 65B - 86.09\n",
      "Falcon 40B - 85.3\n",
      "Falcon 180B - 88.86\n",
      "MPT Instruct 30B - 84.31\n",
      "MPT Instruct 7B - 77.91\n",
      "Yi 6B - 76.42\n",
      "Yi 34B - 85.69\n",
      "GPT-4 - 95.3\n",
      "Gemini Ultra - 87.8\n",
      "From the results presented in Table V it is clear that GPT-4\n",
      "achieves best results for HellaSwag while Davinci-003 is best\n",
      "model for OBQA. It is also good to note that results for OBQA\n",
      "are not reported for all of the models and possibly davinci-003\n",
      "is not the best model achieving highest results on OBQA.\n",
      "Not all models report their performance on all datasets, and\n",
      "because of that, the number of models for which performance\n",
      "is reported in different tables varies.\n",
      "TABLE VI: Symbolic reasoning comparison.\n",
      "Model Cobjects Penguins\n",
      "GPT-NeoX 26 33.56\n",
      "OPT 66B 31.2 28.08\n",
      "Bloomberg GPT 34.8 37.67\n",
      "BLOOM 176B 36.8 40.41\n",
      "PaLM 540B 38 44.5\n",
      "Gopher-280B 49.2 40.6\n",
      "Chinchilla-70B 59.7 48.7\n",
      "PaLM 2 61.2 65.8\n",
      "World knowledge is mostly about general knowledge ques-\n",
      "3. require fine-tuning to be used, they can still benefit from task\n",
      "or data-specific fine-tuning. For example, OpenAI reports that\n",
      "the much smaller GPT-3.5 Turbo model can outperform GPT-4\n",
      "when fine-tuned with task specific data 2.\n",
      "Fine-tuning does not need to be performed to a single\n",
      "task though, and there are different approaches to multi-task\n",
      "fine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\n",
      "or more tasks is known to improve results and reduce the\n",
      "complexity of prompt engineering, and it can serve as an\n",
      "2https://platform.openai.com/docs/guides/fine-tuning\n"
     ]
    }
   ],
   "source": [
    "#Creating the database with FAISS\n",
    "faiss_index = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "#Performing a similarity search\n",
    "# Function to retrieve top 3 relevant documents\n",
    "def retrieve_documents(query, top_k=3):\n",
    "    results = faiss_index.similarity_search(query, k=top_k)\n",
    "    return [doc.page_content for doc in results]\n",
    "\n",
    "# Test the system\n",
    "query = \"WGPT-4 performance compared to other models\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "\n",
    "# Print the retrieved documents\n",
    "print(\"\\nTop relevant documents:\")\n",
    "for idx, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{idx}. {doc}\")\n",
    "\n",
    "\n",
    "combined_text = \"\\n\\n\".join(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vicuna-13B is a cost-efficient model ($300 training) outperforming LLaMA and Alpaca in most cases, with its quality assessed by GPT-4. Guanaco models, efficiently finetuned using QLoRA, achieve high performance (99.3% of ChatGPT on Vicuna benchmark). Benchmark results are presented for various models on tasks like HellaSwag and OBQA, showing varying performance with GPT-4 and Davinci-003 often leading in specific areas. Fine-tuning, even for strong models like GPT-4, can further improve performance on specific tasks.\n"
     ]
    }
   ],
   "source": [
    "#Now, creating the LLM model\n",
    "\n",
    "llm = OpenAI(api_key = 'INSERT_GOOGLE_API_KEY_HERE', \n",
    "             base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "\n",
    "\n",
    "prompt = f\"give summary of the following text, only that and no extra text: {combined_text}\"\n",
    "messages = [{'role' : 'system', 'content' : 'You are a helpful assistant that gives concise information.'},\n",
    "            {'role' : 'user', 'content' : prompt}]\n",
    "\n",
    "response = llm.chat.completions.create(model = 'gemini-2.5-flash-preview-04-17', reasoning_effort=\"low\",\n",
    "                                        messages = messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
